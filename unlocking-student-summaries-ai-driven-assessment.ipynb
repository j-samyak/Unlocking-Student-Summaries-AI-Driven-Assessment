{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":53482,"databundleVersionId":6201832,"sourceType":"competition"}],"dockerImageVersionId":30528,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # **Unlocking Student Summaries: AI-Driven Assessment of Writing Skills**","metadata":{}},{"cell_type":"markdown","source":"## Problem  Statement:\nEmpowering educators and learners with an advanced AI solution that evaluates the quality of student-written summaries, gauging their representation of main ideas, language clarity, and fluency. The project aims to revolutionize the assessment of summary writing, making it more efficient and effective.","metadata":{}},{"cell_type":"markdown","source":"## Introduction:\n\nIn today's rapidly evolving educational landscape, the importance of fostering strong writing skills among students cannot be overstated. Among the many facets of writing, summary writing holds a distinct place for its potential to enhance reading comprehension, critical thinking, and overall writing abilities. However, the labor-intensive nature of evaluating student-written summaries has long been a challenge for educators. With the advent of modern technology and Natural Language Processing (NLP), a groundbreaking solution emerges.\n\nThe \"Unlocking Student Summaries\" project seeks to harness the power of AI to address this challenge effectively. We are presented with a unique opportunity to develop a model that can accurately assess the quality of student summaries, offering immediate feedback and freeing up educators to focus on teaching rather than time-consuming grading. This innovation is made possible by leveraging NLP techniques, machine learning models, and advanced text processing tools.\n\nBy utilizing a dataset of real student summaries, this project aspires to bring a transformative change in the field of education. CommonLit, a nonprofit education technology organization, spearheads this mission, dedicated to ensuring that all students, particularly those in Title I schools, graduate with the necessary skills for success in higher education and beyond. This collaboration with educational organizations embodies the essence of using technology to bridge educational gaps and provide students with more opportunities to develop their summarization, reading comprehension, critical thinking, and writing skills.\n\nThis project's significance extends beyond just the classroom. It contributes to a more inclusive and equitable education system by providing tools to assess and improve writing skills, which are critical for academic and professional success. Through innovative language models and AI-driven evaluation, we aim to change the way summaries are assessed, bringing efficiency and objectivity into the educational landscape. The journey to \"Unlocking Student Summaries\" promises to revolutionize the assessment of student writing, fostering the growth and development of learners across the globe.","metadata":{}},{"cell_type":"markdown","source":"# Loading the Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport spacy\nimport gensim\nimport re\n\n# Initialize NLTK's stopwords and stemmer\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n\n# Load spaCy's English tokenizer, tagger, parser, NER, and word vectors\nnlp = spacy.load('en_core_web_sm')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:45:42.997678Z","iopub.execute_input":"2023-10-30T14:45:42.998504Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Collection and Cleaning","metadata":{}},{"cell_type":"markdown","source":"### Loading the data sets into DataFrame using Pandas","metadata":{}},{"cell_type":"code","source":"df_prompts_test=pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\")\ndf_prompts_train=pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv\")\ndf_summaries_test=pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\")\ndf_summaries_train=pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_prompts_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging the two training datasets, summaries_train and prompts_train as df.","metadata":{}},{"cell_type":"code","source":"df = pd.merge(df_prompts_train, df_summaries_train, on=\"prompt_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging the two test datasets, summaries_test and prompts_test as df_test.","metadata":{}},{"cell_type":"code","source":"df_test = pd.merge(df_prompts_test, df_summaries_test, on=\"prompt_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set \"student id\" as the index\ndf.set_index(\"student_id\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Feature Engineering\nExtracting linguistic features such as the number of nouns, verbs, adjectives, and the number of sentences from the summaries. Clean and format the data for further analysis.","metadata":{}},{"cell_type":"code","source":"# Function to count verbs in a text\ndef count_verbs(text):\n    tokens = word_tokenize(text)\n    tags = pos_tag(tokens)\n    verb_count = len([word for word, pos in tags if pos.startswith('VB')])\n    return verb_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"verb_count_pq\"] = df[\"prompt_question\"].apply(count_verbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"verb_count_pt\"] = df[\"prompt_text\"].apply(count_verbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"verb_count_txt\"] = df[\"text\"].apply(count_verbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the number of nouns in each of the texts","metadata":{}},{"cell_type":"code","source":"# Function to count nouns in a text\ndef count_nouns(text):\n    tokens = word_tokenize(text)\n    tags = pos_tag(tokens)\n    noun_count = len([word for word, pos in tags if pos.startswith('NN')])\n    return noun_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df['noun_count'] contains the number of nouns in each row of the 'prompt_question' column\n","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"noun_count_pq\"] = df[\"prompt_question\"].apply(count_nouns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df['noun_count_pt'] contains the number of nouns in each row of the 'prompt_text' column","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"noun_count_pt\"] = df[\"prompt_text\"].apply(count_nouns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df['noun_count_txt'] contains the number of nouns in each row of the 'text' column","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"noun_count_txt\"] = df[\"text\"].apply(count_nouns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to count adjectives in a text\ndef count_adjectives(text):\n    tokens = word_tokenize(text)\n    tags = pos_tag(tokens)\n    adjective_count = len([word for word, pos in tags if pos.startswith('JJ')])\n    return adjective_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df[\"adj_count_pq\"] contains the number of adjectives in each row of the 'prompt_question' column","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"adj_count_pq\"] = df[\"prompt_question\"].apply(count_adjectives)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df[\"adj_count_pt\"] contains the number of adjectives in each row of the 'prompt_text' column\n","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'prompt_text' column\ndf[\"adj_count_pt\"] = df[\"prompt_text\"].apply(count_adjectives)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df[\"adj_count_txt\"] contains the number of adjectives in each row of the 'text' column","metadata":{}},{"cell_type":"code","source":"# Apply the function to the 'text' column\ndf[\"adj_count_txt\"] = df[\"text\"].apply(count_adjectives)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to count sentences\ndef count_sentences(text):\n    sentences = nltk.sent_tokenize(text)\n    return len(sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"sent_count_pq\"] = df[\"prompt_question\"].apply(count_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_text' column\ndf[\"sent_count_pt\"] = df[\"prompt_text\"].apply(count_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'text' column\ndf[\"sent_count_txt\"] = df[\"text\"].apply(count_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n# Function to count punctuation in a text\ndef count_punctuation(text):\n    tokens = word_tokenize(text)\n    tags = pos_tag(tokens)\n    punctuation_count = len([word for word, pos in tags if pos in string.punctuation])\n    return punctuation_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"punct_count_pq\"] = df[\"prompt_question\"].apply(count_punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_text' column\ndf[\"punct_count_pt\"] = df[\"prompt_text\"].apply(count_punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'text' column\ndf[\"punct_count_txt\"] = df[\"text\"].apply(count_punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to count stop words in a text\ndef count_stop_words(text):\n    # Define a list of stop words (you can customize this list)\n    stop_words = set(['the', 'and', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'at'])\n    tokens = word_tokenize(text)\n    tags = pos_tag(tokens)\n    stop_words_count = len([word for word, pos in tags if word.lower() in stop_words])\n    return stop_words_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_question' column\ndf[\"stw_count_pq\"] = df[\"prompt_question\"].apply(count_stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'prompt_text' column\ndf[\"stw_count_pt\"] = df[\"prompt_text\"].apply(count_stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the 'text' column\ndf[\"stw_count_txt\"] = df[\"text\"].apply(count_stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num = df[[\n    \"content\", \"wording\", \"verb_count_pq\", \"verb_count_pt\", \"verb_count_txt\", \n     \"noun_count_pq\", \"noun_count_pt\", \"noun_count_txt\", \n     \"adj_count_pq\", \"adj_count_pt\", \"adj_count_txt\",\n     \"sent_count_pq\",\"sent_count_pt\", \"sent_count_txt\",\n    \"punct_count_pq\", \"punct_count_pt\", \"punct_count_txt\",\n    \"stw_count_pq\", \"stw_count_pt\", \"stw_count_txt\"\n]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 3x4 grid of subplots with a total figure size of (20, 12)\nfig, ax = plt.subplots(3, 5, figsize=(20, 12))\n\n# Plot 1: Distribution plot for 'content'\nsns.histplot(data=df, x=\"content\", kde=True, ax=ax[0][0])\nax[0][0].set_title(\"Distribution of Content\")\n\n# Plot 2: Distribution plot for 'wording'\nsns.histplot(data=df, x=\"wording\", kde=True, ax=ax[0][1])\nax[0][1].set_title(\"Distribution of Wording\")\n\n# Plot 3: Distribution plot for 'verb_count_pq'\nsns.histplot(data=df, x=\"verb_count_pq\", kde=True, ax=ax[0][2])\nax[0][2].set_title(\"Distribution of Verb Count in Prompt Question\")\n\n# Plot 4: Distribution plot for 'verb_count_pt'\nsns.histplot(data=df, x=\"verb_count_pt\", kde=True, ax=ax[0][3])\nax[0][3].set_title(\"Distribution of Verb Count in Prompt Text\")\n\n# Plot 5: Distribution plot for 'verb_count_txt'\nsns.histplot(data=df, x=\"verb_count_txt\", kde=True, ax=ax[0][4])\nax[0][4].set_title(\"Distribution of Verb Count in Text\")\n\n# Plot 6: Distribution plot for 'noun_count_pq'\nsns.histplot(data=df, x=\"noun_count_pq\", kde=True, ax=ax[1][0])\nax[1][0].set_title(\"Distribution of Noun Count in Prompt Question\")\n\n# Plot 7: Distribution plot for 'noun_count_pt'\nsns.histplot(data=df, x=\"noun_count_pt\", kde=True, ax=ax[1][1])\nax[1][1].set_title(\"Distribution of Noun Count in Prompt Text\")\n\n# Plot 8: Distribution plot for 'noun_count_txt'\nsns.histplot(data=df, x=\"noun_count_txt\", kde=True, ax=ax[1][2])\nax[1][2].set_title(\"Distribution of Noun Count in Text\")\n\n# Plot 9: Distribution plot for 'adj_count_pq'\nsns.histplot(data=df, x=\"adj_count_pq\", kde=True, ax=ax[1][3])\nax[1][3].set_title(\"Distribution of Adjective Count in Prompt Question\")\n\n# Plot 10: Distribution plot for 'adj_count_pt'\nsns.histplot(data=df, x=\"adj_count_pt\", kde=True, ax=ax[1][4])\nax[1][4].set_title(\"Distribution of Adjective Count in Prompt Text\")\n\n# Plot 11: Distribution plot for 'adj_count_txt'\nsns.histplot(data=df, x=\"adj_count_txt\", kde=True, ax=ax[2][0])\nax[2][0].set_title(\"Distribution of Adjective Count in Text\")\n\n# Plot 12: Distribution plot for 'sent_count_pq'\nsns.histplot(data=df, x=\"sent_count_pq\", kde=True, ax=ax[2][1])\nax[2][1].set_title(\"Distribution of Sentence Count in Prompt Question\")\n             \n# Plot 13: Distribution plot for 'sent_count_pq'\nsns.histplot(data=df, x=\"sent_count_pt\", kde=True, ax=ax[2][2])\nax[2][2].set_title(\"Distribution of Sentence Count in Prompt Text\")\n             \n# Plot 14: Distribution plot for 'sent_count_pq'\nsns.histplot(data=df, x=\"sent_count_txt\", kde=True, ax=ax[2][3])\nax[2][3].set_title(\"Distribution of Sentence Count in  Text\")\n\n# Remove the empty subplot\nfig.delaxes(ax[2][4])\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plots\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_word = df_num.drop(columns=[\"content\", \"wording\"]).corrwith(df_num[\"wording\"])\ncor_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_content = df_num.drop(columns=[\"content\", \"wording\"]).corrwith(df_num[\"content\"])\ncor_content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatter plots of high correlating data against content","metadata":{}},{"cell_type":"code","source":"# Create subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Plot verb_count_txt against content using Seaborn\nsns.scatterplot(data=df, x=\"verb_count_txt\", y=\"content\", ax=axs[0, 0])\naxs[0, 0].set_title(\"Scatterplot of verb_count_txt against content\")\naxs[0, 0].set_xlabel(\"verb_count_txt\")\naxs[0, 0].set_ylabel(\"content\")\n\n# Plot noun_count_txt against content using Seaborn\nsns.scatterplot(data=df, x=\"noun_count_txt\", y=\"content\", ax=axs[0, 1])\naxs[0, 1].set_title(\"Scatterplot of noun_count_txt against content\")\naxs[0, 1].set_xlabel(\"noun_count_txt\")\naxs[0, 1].set_ylabel(\"content\")\n\n# Plot adj_count_txt against content using Seaborn\nsns.scatterplot(data=df, x=\"adj_count_txt\", y=\"content\", ax=axs[1, 0])\naxs[1, 0].set_title(\"Scatterplot of adj_count_txt against content\")\naxs[1, 0].set_xlabel(\"adj_count_txt\")\naxs[1, 0].set_ylabel(\"content\")\n\n# Plot sent_count_txt against content using Seaborn\nsns.scatterplot(data=df, x=\"sent_count_txt\", y=\"content\", ax=axs[1, 1])\naxs[1, 1].set_title(\"Scatterplot of sent_count_txt against content\")\naxs[1, 1].set_xlabel(\"sent_count_txt\")\naxs[1, 1].set_ylabel(\"content\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatter plots of high correlating data against wordings","metadata":{}},{"cell_type":"code","source":"# Create subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Plot verb_count_txt against wordings using Seaborn\nsns.scatterplot(data=df, x=\"verb_count_txt\", y=\"wording\", ax=axs[0, 0])\naxs[0, 0].set_title(\"Scatterplot of verb_count_txt against wording\")\naxs[0, 0].set_xlabel(\"verb_count_txt\")\naxs[0, 0].set_ylabel(\"wording\")\n\n# Plot noun_count_txt against wordings using Seaborn\nsns.scatterplot(data=df, x=\"noun_count_txt\", y=\"wording\", ax=axs[0, 1])\naxs[0, 1].set_title(\"Scatterplot of noun_count_txt against wording\")\naxs[0, 1].set_xlabel(\"noun_count_txt\")\naxs[0, 1].set_ylabel(\"wording\")\n\n# Plot adj_count_txt against wordings using Seaborn\nsns.scatterplot(data=df, x=\"adj_count_txt\", y=\"wording\", ax=axs[1, 0])\naxs[1, 0].set_title(\"Scatterplot of adj_count_txt against wording\")\naxs[1, 0].set_xlabel(\"adj_count_txt\")\naxs[1, 0].set_ylabel(\"wording\")\n\n# Plot sent_count_txt against wordings using Seaborn\nsns.scatterplot(data=df, x=\"sent_count_txt\", y=\"wording\", ax=axs[1, 1])\naxs[1, 1].set_title(\"Scatterplot of sent_count_txt against wording\")\naxs[1, 1].set_xlabel(\"sent_count_txt\")\naxs[1, 1].set_ylabel(\"wording\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selction and Model Building\nIn this step, we will:\n\n* Do our feature selection and split into Train test data using test size as 20% of the entire dataset\n* Build the baseline models\n* Experiment with a variety of machine learning models including Decision Trees, XGBoost, Lasso Regression, and other common models.\n* Evaluate model performance using appropriate metrics such as accuracy, precision, and recall.\n* Identify the best-performing model for the task of summary evaluation.","metadata":{}},{"cell_type":"markdown","source":"## Importing the relevant Libraries for the Model Building","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"features= [\"verb_count_pq\", \"verb_count_pt\", \"verb_count_txt\", \n             \"noun_count_pq\", \"noun_count_pt\", \"noun_count_txt\", \n             \"adj_count_pq\", \"adj_count_pt\", \"adj_count_txt\",\n           \"sent_count_pq\",\"sent_count_pt\", \"sent_count_txt\",\n           \"punct_count_pq\", \"punct_count_pt\", \"punct_count_txt\",\n            \"stw_count_pq\", \"stw_count_pt\", \"stw_count_txt\"]\nX=df[features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_1=\"wording\"\ny_word=df[target_1]\ntarget_2=\"content\"\ny_content=df[target_2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data into Train and test Data set","metadata":{}},{"cell_type":"code","source":"# Load your dataset and split it into features (X) and target (y)\nX_train, X_test, y_train_content, y_test_content, y_train_word, y_test_word = train_test_split(X, y_content, y_word, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the baseline models","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_word_mean=y_train_word.mean()\ny_word_pred_baseline=[y_word_mean]*len(y_train_word)\nmae_baseline_word=mean_absolute_error(y_train_word,y_word_pred_baseline)\nrmse_baseline_word=np.sqrt(mean_squared_error(y_train_word,y_word_pred_baseline))\nprint(\"Baseline MAE Wording:\",round(mae_baseline_word, 2))\nprint(\"Baseline RMSE Wording:\",round(rmse_baseline_word, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The baseline **Mean Absolute Error (MAE)** for word score prediction is 0.82, indicating the average absolute difference between predicted word scores and actual word scores.\n\nThe baseline **Root Mean Square Error (RMSE)** for word score prediction is 1.03, reflecting the square root of the average squared differences between predicted word scores and actual word scores.\n\nThese baseline error metrics serve as a reference point to gauge the performance of subsequent models, with the goal of achieving lower MAE and RMSE values as we enhance our summary evaluation system.","metadata":{}},{"cell_type":"code","source":"y_content_mean=y_train_content.mean()\ny_content_pred_baseline=[y_content_mean]*len(y_train_content)\nmae_baseline_word=mean_absolute_error(y_train_content,y_content_pred_baseline)\nrmse_baseline_word=np.sqrt(mean_squared_error(y_train_content,y_content_pred_baseline))\nprint(\"Baseline MAE Content:\",round(mae_baseline_word, 2))\nprint(\"Baseline RMSE Content:\",round(rmse_baseline_word, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Baseline MAE (Mean Absolute Error) for Content Score Prediction: 0.82**\nThe baseline MAE of 0.82, when applied to content score prediction, represents the average absolute difference between the actual content quality assessment scores and the predicted scores generated by our initial model. In this context, a MAE of 0.82 implies that, on average, the model's predictions deviate by 0.82 units from the actual content quality assessment scores. Lower MAE values would indicate that the model's predictions are closer to the true content scores, suggesting room for improvement to minimize this error and enhance the accuracy of content evaluation.\n\n**Baseline RMSE (Root Mean Square Error) for Content Score Prediction: 1.04**\nThe baseline RMSE of 1.04, in the context of content score prediction, represents the square root of the average squared differences between the actual content quality assessment scores and the predicted scores generated by our model. RMSE considers both the magnitude and direction of errors. A RMSE of 1.04 suggests that, on average, the model's predictions deviate by 1.04 units from the true content quality assessment scores. As with MAE, lower RMSE values are desirable and indicate better model performance. The baseline RMSE of 1.04 signals the need for further model optimization to reduce the error and enhance the accuracy of content score predictions.\n\nThese baseline error metrics serve as a reference point for evaluating the performance of future models aimed at content quality assessment. The goal is to achieve lower MAE and RMSE values through model enhancements and refinements.","metadata":{}},{"cell_type":"markdown","source":"# Training and Evaluating the model using all the variables\nHere we will:\n* Experiment with a variety of machine learning models including Decision Trees, XGBoost, Lasso Regression, and other common models.\n* Evaluate model performance using appropriate metrics such as Mean Absolute error(MAE) and Root Mean square Error(RMSE)\n* Identify the best-performing model for the task of summary evaluation.","metadata":{}},{"cell_type":"code","source":"# Define a list of models\nmodels = [\n    LinearRegression(),\n    Lasso(alpha=0.1),\n    Ridge(alpha=0.1),\n    SVR(kernel=\"linear\"),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n]\n\n# Iterate over models\nfor model in models:\n    # Create an instance of the model\n    reg = model\n    \n    # Fit the model to the training data\n    reg.fit(X_train,y_train_content )\n    \n    # Make predictions on the test data\n    y_pred_content = reg.predict(X_test)\n    \n    # Calculate evaluation metrics\n    mae_content = mean_absolute_error(y_test_content, y_pred_content)\n    rmse_content = np.sqrt(mean_squared_error(y_test_content, y_pred_content))\n    \n    # Print model name and evaluation metrics\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"MAE Content: {mae_content:.2f}\")\n    print(f\"RMSE Content: {rmse_content:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The evaluation compares the performance of several machine learning models for predicting content scores based on various variables. Here's a summary based on the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) metrics:\n\nThe models tested are:\n- Linear Regression\n- Lasso\n- Ridge\n- Support Vector Regressor (SVR)\n- Decision Tree Regressor\n- Random Forest Regressor\n- Gradient Boosting Regressor\n\nThe results indicate that the Gradient Boosting Regressor performs the best among the models evaluated, showcasing the lowest MAE of 0.38 and RMSE of 0.51. Following closely, the Random Forest Regressor also demonstrates strong performance with an MAE of 0.39 and RMSE of 0.52. These models outperform the others in accurately predicting content scores based on the given variables.\n\nThe Gradient Boosting Regressor, in particular, stands out for its ability to handle complex relationships within the data and generate highly accurate predictions, making it the recommended choice for this particular prediction task based on the provided evaluation metrics.","metadata":{}},{"cell_type":"markdown","source":"The model on the Wording as the target","metadata":{}},{"cell_type":"code","source":"# Define a list of models\nmodels = [\n    LinearRegression(),\n    Lasso(alpha=0.1),\n    Ridge(alpha=0.1),\n    SVR(kernel=\"linear\"),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n]\n\n# Iterate over models\nfor model in models:\n    # Create an instance of the model\n    reg_word = model\n    \n    # Fit the model to the training data\n    reg_word.fit(X_train,y_train_word)\n    \n    # Make predictions on the test data\n    y_pred_word = reg_word.predict(X_test)\n    \n    # Calculate evaluation metrics\n    mae_word = mean_absolute_error(y_test_word, y_pred_word)\n    rmse_word = np.sqrt(mean_squared_error(y_test_word, y_pred_word))\n    \n    # Print model name and evaluation metrics\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"MAE Word: {mae_word:.2f}\")\n    print(f\"RMSE Word: {rmse_word:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predictive models were evaluated based on their performance in estimating a summary wordings score. Among the models tested, the GradientBoostingRegressor demonstrated the most accurate predictions, achieving the lowest Mean Absolute Error (MAE) of 0.54 and Root Mean Squared Error (RMSE) of 0.72. Following closely behind, the RandomForestRegressor also displayed strong performance with an MAE of 0.55 and an RMSE of 0.74. These results suggest that both GradientBoostingRegressor and RandomForestRegressor are the top-performing models for predicting the summary wordings score, outperforming other models like LinearRegression, Lasso, Ridge, SVR, DecisionTreeRegressor in terms of accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Using the high correlation variables","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"features_sel= [\"verb_count_txt\", \"noun_count_txt\", \"adj_count_txt\",\"sent_count_txt\", \"punct_count_txt\",\"stw_count_txt\"]\nX_sel=df[features_sel]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the Data Set","metadata":{}},{"cell_type":"code","source":"# Load your dataset and split it into features (X) and target (y)\nX_sel_train, X_sel_test, y_train_content, y_test_content, y_train_word, y_test_word = train_test_split(X_sel, y_content, y_word, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a list of models\nmodels = [\n    LinearRegression(),\n    Lasso(alpha=0.1),\n    Ridge(alpha=0.1),\n    SVR(kernel=\"linear\"),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n]\n\n# Iterate over models\nfor model in models:\n    # Create an instance of the model\n    reg = model\n    \n    # Fit the model to the training data\n    reg.fit(X_sel_train,y_train_content )\n    \n    # Make predictions on the test data\n    y_pred_content = reg.predict(X_sel_test)\n    \n    # Calculate evaluation metrics\n    mae_content_sel = mean_absolute_error(y_test_content, y_pred_content)\n    rmse_content_sel = np.sqrt(mean_squared_error(y_test_content, y_pred_content))\n    \n    # Print model name and evaluation metrics\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"MAE_sel Content: {mae_content_sel:.2f}\")\n    print(f\"RMSE_sel Content: {rmse_content_sel:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the performance metrics for predicting content scores using various regression models, the GradientBoostingRegressor stands out as the best-performing model. It achieved the lowest Mean Absolute Error (MAE) of 0.41 and Root Mean Squared Error (RMSE) of 0.54 among all models tested. This indicates its superior predictive capability in estimating content scores based on highly correlated variables compared to other models like LinearRegression, Lasso, Ridge, SVR, DecisionTreeRegressor, and RandomForestRegressor.","metadata":{}},{"cell_type":"markdown","source":"The model on the Wording as the target","metadata":{}},{"cell_type":"code","source":"# Define a list of models\nmodels = [\n    LinearRegression(),\n    Lasso(alpha=0.1),\n    Ridge(alpha=0.1),\n    SVR(kernel=\"linear\"),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n]\n\n# Iterate over models\nfor model in models:\n    # Create an instance of the model\n    reg_word = model\n    \n    # Fit the model to the training data\n    reg_word.fit(X_sel_train,y_train_word)\n    \n    # Make predictions on the test data\n    y_pred_word = reg_word.predict(X_sel_test)\n    \n    # Calculate evaluation metrics\n    mae_word_sel = mean_absolute_error(y_test_word, y_pred_word)\n    rmse_word_sel = np.sqrt(mean_squared_error(y_test_word, y_pred_word))\n    \n    # Print model name and evaluation metrics\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"MAE_sel_word: {mae_word_sel:.2f}\")\n    print(f\"RMSE_sel_word: {rmse_word_sel:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the performance metrics for predicting summary wording scores using different regression models, the GradientBoostingRegressor stands out as the best-performing model. It achieved the lowest Mean Absolute Error (MAE) of 0.58 and Root Mean Square Error (RMSE) of 0.78, indicating its superior predictive accuracy compared to other models tested. This suggests that the GradientBoostingRegressor is the most effective in capturing the relationships between highly correlated variables and accurately predicting summary wording scores.","metadata":{}},{"cell_type":"markdown","source":"### Best Performing Model\nBased on these performance metrics, it appears that the GradientBoostingRegressor is the most promising model, as it has the lowest MAE and RMSE. It indicates that this model is providing the best overall performance in evaluating the quality of student summaries. Also it appears that the models built on highly correlated variables outperformed the ones on all the variables. With this result we shall be performing hyperparameter tunning on Gradient Boosting Regressor using the Highly Correlated Variables.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Tunning\nFine-tuning hyperparameters for machine learning models especially the GradientBoostingRegressor is an essential step to optimize its performance. It can be done by using techniques such as Grid Search or Random Search to explore different hyperparameter combinations. Here we will be performing hyperparameter tuning for the GradientBoostingRegressor using **Grid Search**.","metadata":{}},{"cell_type":"code","source":"# For Word Prediction:\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters you want to tune\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of boosting stages to be used\n    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking to prevent overfitting\n}\n\n# Create the GradientBoostingRegressor\nreg_word = GradientBoostingRegressor()\n\n# Use GridSearchCV to find the best combination of hyperparameters\ngrid_search = GridSearchCV(reg_word, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_sel_train, y_train_word)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Create a GradientBoostingRegressor with the best hyperparameters\nbest_reg_word = GradientBoostingRegressor(**best_params)\n\n# Fit the model to the training data\nbest_reg_word.fit(X_sel_train, y_train_word)\n\n# Make predictions on the test data\ny_pred_word = best_reg_word.predict(X_sel_test)\n\n# Calculate evaluation metrics\nmae_word_sel = mean_absolute_error(y_test_word, y_pred_word)\nrmse_word_sel = np.sqrt(mean_squared_error(y_test_word, y_pred_word))\n\n# Print evaluation metrics\nprint(\"MAE_sel_word:\", mae_word_sel)\nprint(\"RMSE_sel_word:\", rmse_word_sel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best hyperparameters for the GradientBoostingRegressor model, as determined, are as follows: learning rate of 0.1,and 100 estimators.\n\nWith these hyperparameters, the model achieved a Mean Absolute Error (MAE) of 0.583 and a Root Mean Square Error (RMSE) of 0.777 when predicting summary wording scores. These results further validate the effectiveness of the chosen hyperparameters, as they significantly contribute to the model's accurate predictions and performance.","metadata":{}},{"cell_type":"code","source":"# For Content\n# Define the hyperparameters you want to tune\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of boosting stages to be used\n    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking to prevent overfitting\n}\n\n# Create the GradientBoostingRegressor\nreg_content = GradientBoostingRegressor()\n\n# Use GridSearchCV to find the best combination of hyperparameters\ngrid_search = GridSearchCV(reg_content, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_sel_train, y_train_content)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Create a GradientBoostingRegressor with the best hyperparameters\nbest_reg_content = GradientBoostingRegressor(**best_params)\n\n# Fit the model to the training data\nbest_reg_content.fit(X_sel_train, y_train_content)\n\n# Make predictions on the test data\ny_pred_content = best_reg_content.predict(X_sel_test)\n\n# Calculate evaluation metrics\nmae_content_sel = mean_absolute_error(y_test_content, y_pred_content)\nrmse_content_sel = np.sqrt(mean_squared_error(y_test_content, y_pred_content))\n\n# Print evaluation metrics\nprint(\"MAE_sel_content:\", mae_content_sel)\nprint(\"RMSE_sel_content:\", rmse_content_sel)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The GradientBoostingRegressor model's best hyperparameters were found to be {'learning_rate': 0.1, 'n_estimators': 100}. With these parameters, it achieved an impressive Mean Absolute Error (MAE) of 0.408 and Root Mean Square Error (RMSE) of 0.537 in predicting content quality. This signifies the model's robust performance in accurately assessing content quality, demonstrating its efficacy in handling the intricacies of the data with superior predictive capability.","metadata":{}},{"cell_type":"markdown","source":"# WORD EMBEDDING METHOD\nIn this step we will be:\n* Perform text preprocessing including tokenization, stop word removal, and stemming using the Porter Stemmer.\n* Utilize the TF-IDF vectorization technique to convert text data into numerical features.\n* Normalize the vectorized features to ensure consistency and facilitate modeling.\n* Select our features\n* Train the selected model on the preprocessed and vectorized data.\n* Fine-tune hyperparameters to optimize the model's performance.","metadata":{}},{"cell_type":"code","source":"df_emb = pd.merge(df_prompts_train, df_summaries_train, on=\"prompt_id\")\ndf_emb_test = pd.merge(df_prompts_test, df_summaries_test, on=\"prompt_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_emb.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    words = text.split()\n    # Remove stopwords and apply stemming\n    words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n    return ' '.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply th function on the test and train data sets","metadata":{}},{"cell_type":"code","source":"\ndf_emb[\"prompt_question\"]= df_emb[\"prompt_question\"].apply(preprocess_text)\ndf_emb[\"prompt_title\"]= df_emb[\"prompt_title\"].apply(preprocess_text)\ndf_emb[\"prompt_text\"]= df_emb[\"prompt_text\"].apply(preprocess_text)\ndf_emb[\"text\"]= df_emb[\"text\"].apply(preprocess_text)\ndf_emb_test[\"prompt_question\"]= df_emb_test[\"prompt_question\"].apply(preprocess_text)\ndf_emb_test[\"prompt_title\"]= df_emb_test[\"prompt_title\"].apply(preprocess_text)\ndf_emb_test[\"prompt_text\"]= df_emb_test[\"prompt_text\"].apply(preprocess_text)\ndf_emb_test[\"text\"]= df_emb_test[\"text\"].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Vectorization","metadata":{}},{"cell_type":"code","source":"# List of maximum values to iterate through\nmax_values = [500, 1000, 1500, 2000, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n\nfor max_features in max_values:\n    # Initialize the tfidf\n    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying The Vectorization on the processed Data Frame","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import hstack\n\n# Initialize an empty sparse matrix\nX_tfidf = None\n\n# Define the text columns for TF-IDF vectorization\ntext_columns = [\"text\"]\n\n# Iterate through each text column and perform TF-IDF vectorization\nfor col in text_columns:\n    tfidf_matrix = tfidf_vectorizer.fit_transform(df_emb[col])\n    if X_tfidf is None:\n        X_tfidf = tfidf_matrix\n    else:\n        # Concatenate the sparse matrices horizontally\n        X_tfidf = hstack([X_tfidf, tfidf_matrix])\n\n# X_tfidf now contains the combined TF-IDF features for all text columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting the Predicting Features","metadata":{}},{"cell_type":"code","source":"X= X_tfidf\ny_content = df_emb[\"content\"].values\ny_wording = df_emb[\"wording\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_content_train, y_content_test, y_wording_train, y_wording_test = train_test_split(\n    X, y_content, y_wording, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Machine Learning Models","metadata":{}},{"cell_type":"code","source":"# List of maximum values to iterate through\nmax_values = [500, 1000, 1500, 2000, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n\nfor max_features in max_values:\n    # Initialize the tfidf\n    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n    \n    # Applying the Vectorization on the processed data frame\n    X_tfidf = None\n\n    text_columns = [\"text\"]\n\n    for col in text_columns:\n        tfidf_matrix = tfidf_vectorizer.fit_transform(df_emb[col])\n        if X_tfidf is None:\n            X_tfidf = tfidf_matrix\n        else:\n            X_tfidf = hstack([X_tfidf, tfidf_matrix])\n\n    X = X_tfidf\n    y_content = df_emb[\"content\"].values\n    y_wording = df_emb[\"wording\"].values\n\n    # Train-Test Split\n    X_train, X_test, y_content_train, y_content_test, y_wording_train, y_wording_test = train_test_split(\n        X, y_content, y_wording, test_size=0.2, random_state=42)\n\n    # Build the Machine Learning Model\n    content_model = GradientBoostingRegressor()\n    content_model.fit(X_train, y_content_train)\n\n    wording_model = GradientBoostingRegressor()\n    wording_model.fit(X_train, y_wording_train)\n\n    # Evaluate Models\n    content_predictions = content_model.predict(X_test)\n    content_rmse = np.sqrt(mean_squared_error(y_content_test, content_predictions))\n    #mean_squared_error(y_content_test, content_predictions, squared=False)\n    content_mae= mean_absolute_error(y_content_test, content_predictions)\n\n    wording_predictions = wording_model.predict(X_test)\n    wording_rmse = mean_squared_error(y_wording_test, wording_predictions, squared=False)\n    wording_mae= mean_absolute_error(y_wording_test, wording_predictions)\n\n    print(f\"Max Features: {max_features}\")\n    print(f\"Content RMSE: {content_rmse}\")\n    print(f\"Wording RMSE: {wording_rmse}\")\n    print(f\"Content MAE: {content_mae}\")\n    print(f\"Wording MAE: {wording_mae}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results of the Gradient Boosting Regressor on vectorized preprocessed text using various maximum features of TFIDF for both content and wording scores of the summaries are as follows:\n\nWhen considering the RMSE (Root Mean Square Error), the best performance for content scores is achieved with a maximum feature count of 5000, resulting in a RMSE of approximately 0.6017. For wording scores, the lowest RMSE is also obtained with 5000 maximum features, at around 0.7644.\n\nIn terms of MAE (Mean Absolute Error), the optimal maximum feature count for content scores is 5000, with a MAE of roughly 0.4712. For wording scores, the lowest MAE is also found with 5000 maximum features, at approximately 0.6117.\n\nThese results suggest that a maximum feature count of 5000 provides the best balance between accuracy and feature dimensionality reduction when using the Gradient Boosting Regressor for predicting both content and wording scores of summaries based on vectorized preprocessed text.","metadata":{}},{"cell_type":"code","source":"# Initialize the tfidf with the restricted max_features\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\n\nX_tfidf = None\ntext_columns = [\"text\"]\n\nfor col in text_columns:\n    tfidf_matrix = tfidf_vectorizer.fit_transform(df_emb[col])\n    if X_tfidf is None:\n        X_tfidf = tfidf_matrix\n    else:\n        X_tfidf = hstack([X_tfidf, tfidf_matrix])\n\nX = X_tfidf\ny_content = df_emb[\"content\"].values\ny_wording = df_emb[\"wording\"].values\n\n# Train-Test Split\nX_train, X_test, y_content_train, y_content_test, y_wording_train, y_wording_test = train_test_split(\n    X, y_content, y_wording, test_size=0.2, random_state=42)\n\n# Define the parameter grid to search through\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Example values, you can modify these\n    'learning_rate': [0.05, 0.1, 0.2],  # Example values, you can modify these\n    # Add more parameters to be tuned\n}\n\n# Initialize the Gradient Boosting Regressor\ngbr = GradientBoostingRegressor()\n\n# Perform Grid Search with Cross Validation\ngrid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_content_train)  # Fit the grid search on the content data\n\n\n# Evaluate the best model on test data\ncontent_predictions = grid_search.predict(X_test)\ncontent_rmse = np.sqrt(mean_squared_error(y_content_test, content_predictions))\ncontent_mae= mean_absolute_error(y_content_test, content_predictions)\n\n# Print the best results\nprint(f\"Best Parameters: {grid_search.best_params_}\")\nprint(f\"Best Content RMSE: {content_rmse}\")\nprint(f\"Best Content MAE: {content_mae}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The hyperparameter tuning process optimized the Gradient Boosting Regressor for content predictions, achieving a good balance between model complexity and predictive performance. The selected hyperparameters include a learning rate of 0.1 and 300 estimators (trees) in the ensemble. With a TF-IDF vectorization limited to 5000 features, the model achieved a low RMSE of 0.5517, indicating that, on average, content predictions were within approximately 0.5517 units of the true values. The MAE of 0.4290 further underscores the model's accuracy, showing that the average absolute difference between predicted and actual content values is around 0.4290.\n\nThese results suggest that the combination of hyperparameters and feature constraints has resulted in a well-performing model for content predictions. However, keep in mind that model performance should always be evaluated in the context of the specific problem domain and its requirements.","metadata":{}},{"cell_type":"code","source":"# Initialize the tfidf with the restricted max_features\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\n\nX_tfidf = None\ntext_columns = [\"text\"]\n\nfor col in text_columns:\n    tfidf_matrix = tfidf_vectorizer.fit_transform(df_emb[col])\n    if X_tfidf is None:\n        X_tfidf = tfidf_matrix\n    else:\n        X_tfidf = hstack([X_tfidf, tfidf_matrix])\n\nX = X_tfidf\ny_content = df_emb[\"content\"].values\ny_wording = df_emb[\"wording\"].values\n\n# Train-Test Split\nX_train, X_test, y_content_train, y_content_test, y_wording_train, y_wording_test = train_test_split(\n    X, y_content, y_wording, test_size=0.2, random_state=42)\n\n# Define the parameter grid to search through\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Example values, you can modify these\n    'learning_rate': [0.05, 0.1, 0.2],  # Example values, you can modify these\n    # Add more parameters to be tuned\n}\n\n# Initialize the Gradient Boosting Regressor\ngbr = GradientBoostingRegressor()\n\n# Perform Grid Search with Cross Validation\ngrid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_wording_train)  # Fit the grid search on the content data\n\n\n# Evaluate the best model on test data\nwording_predictions = grid_search.predict(X_test)\nwording_rmse = np.sqrt(mean_squared_error(y_wording_test, wording_predictions))\nwording_mae= mean_absolute_error(y_wording_test, wording_predictions)\n\n# Print the best results\nprint(f\"Best Parameters: {grid_search.best_params_}\")\nprint(f\"Best Wording RMSE: {wording_rmse}\")\nprint(f\"Best Wording MAE: {wording_mae}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the hyperparameter tuning performed for the Gradient Boosting Regressor with TF-IDF vectorization using a maximum of 5000 features, the best parameters identified for predicting the \"wording\" variable were a learning rate of 0.2 and 300 estimators (trees) in the ensemble model.\n\nThe evaluation metrics for the best model achieved the following results:\n\nBest Wording Root Mean Squared Error (RMSE): 0.704\nBest Wording Mean Absolute Error (MAE): 0.561\nThese metrics provide an assessment of the predictive accuracy of the model. The RMSE measures the average magnitude of the errors between predicted and actual values, with a lower RMSE indicating better performance. Meanwhile, the MAE measures the average magnitude of errors in a set of predictions, providing insights into the model's predictive accuracy.\n\nIn summary, the optimized Gradient Boosting model using TF-IDF vectorization with 5000 features, a learning rate of 0.2, and 300 estimators performed well in predicting the \"wording\" variable, achieving low error rates, indicating a relatively accurate predictive model.","metadata":{}}]}